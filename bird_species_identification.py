# -*- coding: utf-8 -*-
"""bird species identification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cK9fnGAtkY67jz90riux46B9bqL2Oc6U
"""

from google.colab import drive
drive.mount('/content/drive')

ls "/content/drive/My Drive"

pip install keras

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.image import imread
import cv2
import random
from os import listdir
from sklearn.preprocessing import LabelBinarizer
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array, array_to_img
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense
from sklearn.model_selection import train_test_split

print(os.listdir(path))

import matplotlib.pyplot as plt
from matplotlib.image import imread
import os
import random

path = '/content/drive/MyDrive/archive_extracted/test/AMERICAN GOLDFINCH/'
image_files = [f for f in os.listdir(path) if f.endswith(('.png', '.jpg', '.jpeg'))]

if image_files:
    rand_img_path = os.path.join(path, random.choice(image_files))
    rand_img = imread(rand_img_path)

    plt.subplot(4, 4, 1)  # Adjust index as needed
    plt.tight_layout()

    # Display the image using imshow
    plt.imshow(rand_img, cmap='gray')

    # Add title and labels
    plt.title('Img')
    plt.xlabel(f'Width: {rand_img.shape[1]}', fontsize=10)
    plt.ylabel(f'Height: {rand_img.shape[0]}', fontsize=10)

    plt.show()
else:
    print("No valid image files found in the directory.")

import os

# Directory where images are stored (e.g., train directory)
image_dir = "/content/drive/MyDrive/archive_extracted/train/"

# Initialize an empty list for labels
label_list = []

# Traverse through the directories
for root, dirs, files in os.walk(image_dir):
    for file in files:
        # Append the name of the sub-directory (the label) to the label list
        label = os.path.basename(root)
        label_list.append(label)

# Print the first few labels
print("All labels:", label_list[:525])

dir= '/content/drive/MyDrive/archive_extracted'
from os import listdir
root_dir = listdir(dir)

image_list, label_list = [ ], [ ]

root_dir = "/home/user/images"

root_dir = "C:/Users/your_user_name/images"

import os
import cv2

# Define the correct path to your image directory
root_dir = "/content/drive/MyDrive/archive_extracted/test"

# Ensure the directory exists before proceeding
if not os.path.exists(root_dir):
    print(f"Directory {root_dir} does not exist!")
else:
    # Reading and converting images to numpy array
    for directory in os.listdir(root_dir):
        dir_path = os.path.join(root_dir, directory)
        if os.path.isdir(dir_path):  # Ensure it's a directory
            for file in os.listdir(dir_path):
                image_path = os.path.join(dir_path, file)
                image = cv2.imread(image_path)
                # Now you can process the image further

import os
import pandas as pd

# Define your directory paths
train_dir = "/content/drive/MyDrive/archive_extracted/train/"
test_dir = "/content/drive/MyDrive/archive_extracted/test/"

# Function to check if a directory exists
def check_directory(path):
    if not os.path.exists(path):
        print(f"Directory {path} does not exist!")
        return False
    return True

# Check if the test directory exists
if check_directory(test_dir):
    print(f"Test directory exists: {test_dir}")
else:
    print(f"Test directory is missing!")

# List and count files in the training directory
if check_directory(train_dir):
    train_files = []

    # Loop through the train directory and get all file paths
    for root, dirs, files in os.walk(train_dir):
        for file in files:
            # Append the full path to the train_files list
            train_files.append(os.path.join(root, file))

    # Convert file paths to DataFrame to count
    train_labels = [os.path.basename(os.path.dirname(path)) for path in train_files]
    train_df = pd.DataFrame(train_labels, columns=["label"])

    # Count the occurrences of each label
    label_counts_train = train_df["label"].value_counts()

    # Print the label counts
    print("Label counts in the training set:")
    print(label_counts_train)

num_classes = len(label_counts_train)
num_classes

image_list[0].shape

import numpy as np
label_list = np.array(label_list)
label_list.shape

import numpy as np
import glob
from sklearn.model_selection import train_test_split

# Get a list of files matching the pattern
label_list = glob.glob('../content/drive/MyDrive/archive_extracted/train/**/*') # Added ** to search recursively in subdirectories

# Convert the list to a NumPy array
label_list = np.array(label_list)

# Check the shape to confirm it has more than one element
print(label_list.shape)

# Now you can use train_test_split if label_list has more than one element
if len(label_list) > 1:
  # Assuming image_list is derived from label_list, replace this with the actual code to create image_list
  image_list = [f.replace('train', 'image') for f in label_list] # Example: Assuming image files are in a folder named 'image' with the same structure as 'train'
  image_list = np.array(image_list)
  x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state =10 )
else:
  print("Error: Not enough files found to perform train_test_split.")

x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state =10 )

x_train = []
y_train = []
x_test = []
y_test = []

import os
import glob
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# Get the list of image files
image_list = glob.glob('../content/drive/MyDrive/archive_extracted/train/*/*')  # Assuming the images are inside subdirectories

# Extract labels from the folder names
label_list = [os.path.basename(os.path.dirname(image)) for image in image_list]

# Convert the lists to NumPy arrays
image_list = np.array(image_list)
label_list = np.array(label_list)

# Check if there are enough samples for splitting
if len(image_list) > 1 and len(label_list) > 1:
    # Split data into training and testing sets
    x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state=10)

    # Encode the labels
    lb = LabelBinarizer()
    y_train_encoded = lb.fit_transform(y_train)
    y_test_encoded = lb.transform(y_test)

    print(f"Sample training labels: {y_train[:10]}")  # Print the first 10 training labels for inspection
    print(f"Encoded labels: {lb.classes_}")  # Print the encoded class names
else:
    print("Error: Not enough files found to perform train_test_split.")

X_train, x_val, y_train, y_val = train_test_split(x_train, y_train_encoded, test_size=0.2)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(60, 60, 3)))
model.add

!pip show numpy

!pip install --upgrade numpy # Added a '!' to execute this as a shell command

# DOwnload

IMAGE_SIZE = {60,60}

train_directorty='../input/100-bird-species/train'
test_directorty='../input/100-bird-species/test'
val_directory='../input/100-bird-species/valid'

# Use a magic command to run the pip install command in the system shell
!pip install tensorflow

# Use a list instead of a set for IMAGE_SIZE
IMAGE_SIZE = [60,60]

import tensorflow as tf # imports the tensorflow library and assigns it to the variable tf

vgg = tf.keras.applications.VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False) # You might need to import the VGG16 class from tensorflow.keras.applications
for layer in vgg.layers:
    layer.trainable = False

folders = glob('../input/100-bird-species/train/*')
len(folders)

from glob import glob # import the glob function from the glob module

folders = glob('../content/drive/MyDrive/archive_extracted/train/*') # call the glob function imported from the glob module
len(folders)

import glob # imports the glob module

folders = glob.glob('../content/drive/MyDrive/archive_extracted/train/*') # call the glob function using the module name
len(folders)

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
num_classes = 525
model = Sequential()
model.add(Conv2D(8, (3,3), padding='same', input_shape=(224,224,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(16, (3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005), metrics = ['accuracy'])

num_classes = 525
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3), kernel_regularizer=regularizers.l2(0.001)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout after MaxPooling
    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout after MaxPooling
    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout after MaxPooling
    Flatten(),
    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    Dropout(0.5),  # Dropout before the output layer
    Dense(num_classes, activation='softmax')
    ])

# ... (rest of your code for data loading and training)

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras import regularizers # Import regularizers

num_classes = 525
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3), kernel_regularizer=regularizers.l2(0.001)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout after MaxPooling
    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout after MaxPooling
    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),  # Dropout after MaxPooling
    Flatten(),
    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    Dropout(0.5),  # Dropout before the output layer
    Dense(num_classes, activation='softmax')
])

# ... (rest of your code for data loading and training)

num_samples = 1000
data = np.random.rand(num_samples, 224, 224, 3)
labels = np.random.randint(0, num_classes, num_samples)
x_train, x_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)
x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)
y_train = to_categorical(y_train, num_classes=num_classes)
y_val = to_categorical(y_val, num_classes=num_classes)
y_test = to_categorical(y_test, num_classes=num_classes)
model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.0005), metrics=['accuracy'])
epochs = 50
batch_size = 128
history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))

import numpy as np
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split # Import train_test_split
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras import regularizers

# ... (rest of your code)

model.save('/content/drive/MyDrive/archive_extracted.h5')

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.plot(history.history['accuracy'], color='r')
plt.plot(history.history['val_accuracy'], color='b')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'val'])
plt.show()

plt.figure(figsize=(12, 5))
plt.plot(history.history['loss'],color='r')
plt.plot(history.history['val_loss'],color='b')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train','val'])
plt.show()

scores = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {scores[1]*100}")

y_pred = model.predict(x_test)

y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

from keras.preprocessing.image import array_to_img
import numpy as np
from IPython.display import display
img_array = np.array(x_test[5], dtype=np.uint8)
img = array_to_img(img_array)
display(img)

from keras.preprocessing.image import array_to_img, load_img
import numpy as np
from IPython.display import display
from PIL import Image
import matplotlib.pyplot as plt
def display_image(image_data):
  """Displays an image in Colab, handling file paths and normalized data.

  Args:
    image_data: Either a file path (string) or a NumPy array representing the image.
  """
  try:
    if isinstance(image_data, str):
        img = load_img(image_data)
    elif isinstance(image_data, np.ndarray):
        if image_data.min() >= 0 and image_data.max() <= 1:
            image_data = (image_data * 255).astype(np.uint8)
        if image_data.ndim == 2:
            image_data = np.stack((image_data,) * 3, axis=-1)
    else:
        raise TypeError("Invalid image data type. Expected string or NumPy array.")
    plt.imshow(img)
    plt.axis('off')
    plt.show()
  except Exception as e:
    print(f"Error displaying image: {e}")
display_image(x_test[5])

print(x_test[5].shape)
print(x_test[5].dtype)

print(x_test[5].min(), x_test[5].max())

from keras.preprocessing.image import array_to_img
import numpy as np
import glob
import matplotlib.pyplot as plt

image_list = glob.glob('../content/drive/MyDrive/archive_extracted/train/*/*')
img_array = x_test[1]

# Ensure the image data is in the correct range and data type
if img_array.min() < 0 or img_array.max() > 1:
    img_array = (img_array - img_array.min()) / (img_array.max() - img_array.min())  # Normalize to 0-1
    img_array = (img_array * 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8
else:
    img_array = (img_array * 255).astype(np.uint8)  # If already 0-1, just scale and convert

# Check if image is grayscale and convert to RGB if necessary
if img_array.ndim == 2:  # Grayscale (height, width)
    img_array = np.stack((img_array,) * 3, axis=-1)  # Convert to RGB by stacking 3 times

# Try displaying directly with matplotlib (to handle potential channel order issues)
plt.imshow(img_array)
plt.axis('off')
plt.show()

import pickle
from sklearn.preprocessing import LabelBinarizer
import numpy as np
import pandas as pd  # Import pandas for data loading

# **Path to your dataset**
dataset_path = '/content/drive/MyDrive/archive_extracted/birds.csv'  # Replace with the actual path

# Load your training data using pandas
data = pd.read_csv(dataset_path)

# Assuming 'labels' is the column containing the bird names, and checking if it exists
# Replace 'labels' with the actual name of the bird species column in your CSV file
bird_species_column = 'labels'  # or any other relevant column name
if bird_species_column in data.columns:
    y_train = data[bird_species_column].values
else:
    raise KeyError(f"Column '{bird_species_column}' not found in the DataFrame. Please check your CSV file.")

# During Training (after fitting LabelBinarizer to y_train)
lb = LabelBinarizer()
lb.fit(y_train)  # Fit to your training labels
labels = lb.classes_  # Store the class names
with open('label_binarizer.pkl', 'wb') as f:
    pickle.dump(lb, f)  # Save LabelBinarizer

# During Prediction:
with open('label_binarizer.pkl', 'rb') as f:
    lb = pickle.load(f)  # Load LabelBinarizer
labels = lb.classes_  # Access class names

# ... (Your code to make predictions using your model, resulting in y_pred) ...

# Replace with your prediction results
# Example for demonstration:
y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.1, 0.8]])

y_pred_classes = np.argmax(y_pred, axis=1)  # Predicted class indices
predicted_bird_names = [labels[i] for i in y_pred_classes]  # Get corresponding bird names

# Assuming you have the predicted class index in y_pred_classes
bird_name = labels[y_pred_classes[0]]
print(bird_name)

labels = lb.classes_
labels = lb.classes_
print(labels)
print("orginally : ",labels[np.argmax(y_test[1])])
print("predicted : ",labels[y_pred_classes[1]])

!pip install scikit-learn



labels = lb.classes_ # Get the classes after fitting

predicted_bird_names = [original_bird_names[i] for i in y_pred_classes]

from sklearn.preprocessing import LabelBinarizer
import pickle

# Try to load the LabelBinarizer object from the file
try:
    with open('label_binarizer.pkl', 'rb') as f:
        lb = pickle.load(f)
except FileNotFoundError:
    # If the file doesn't exist, create a new LabelBinarizer
    lb = LabelBinarizer()
    # You would fit it here to your training data if available
    # lb.fit(your_training_data)
    with open('label_binarizer.pkl', 'wb') as f:
        pickle.dump(lb, f)

# Now that 'lb' is defined, you can access its attributes
labels = lb.classes_  # Get the classes after fitting
print(labels)

# Assuming 'lb' was an instance of LabelBinarizer
from sklearn.preprocessing import LabelBinarizer

# ... (Your existing code to load and process data) ...

# If you're loading the labels from a file:
import pickle

try:
    # Attempt to load the LabelBinarizer object from a file
    with open('label_binarizer.pkl', 'rb') as f:
        lb = pickle.load(f)
        # Load the classes if they were saved
        labels = lb.classes_ if hasattr(lb, 'classes_') else None
except FileNotFoundError:
    # If the file doesn't exist, create a new LabelBinarizer
    lb = LabelBinarizer()
    # ... and fit it to your training labels
    # For example: lb.fit(y_train)
    lb.fit(y_train) # Fit the LabelBinarizer to your training labels (y_train)
    labels = lb.classes_  # Store the classes after fitting
    # ... and then save it to be reused along with the LabelBinarizer.
    with open('label_binarizer.pkl', 'wb') as f:
        pickle.dump(lb, f)
        # You might want to save 'labels' as well to a separate file or within the same file.


# If labels were loaded or previously saved then use them; otherwise, get from lb
# This condition is now redundant since 'lb' is always fitted in the 'except' block
# if labels is None:
#     labels = lb.classes_ # Get the classes after fitting

# You should now be able to access the classes using 'labels' variable.
print(labels)



import pickle
from sklearn.preprocessing import LabelBinarizer
import numpy as np
import os  # Import os for file/directory operations

# Path to your directory containing images
# **Make sure this path is correct and exists.**
# The path should point to the parent directory of the bird species folders.
path = '/content/drive/MyDrive/archive_extracted/train/'  # Corrected path

# Get the unique bird species (subdirectories)
bird_species = sorted([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])
# Check if directories exist to avoid the FileNotFoundError
y_train = bird_species  # Use the unique bird species as your training labels

# During Training (after fitting LabelBinarizer to y_train)
lb = LabelBinarizer()
lb.fit(y_train)  # Fit to your training labels
labels = lb.classes_  # Store the class names
with open('label_binarizer.pkl', 'wb') as f:
    pickle.dump(lb, f)  # Save LabelBinarizer

# During Prediction:
with open('label_binarizer.pkl', 'rb') as f:
    lb = pickle.load(f)  # Load LabelBinarizer
labels = lb.classes_  # Access class names

# ... (Your code to make predictions using your model, resulting in y_pred) ...

# Replace with your prediction results
# Example for demonstration:
y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.1, 0.8]])

y_pred_classes = np.argmax(y_pred, axis=1)  # Predicted class indices
predicted_bird_names = [labels[i] for i in y_pred_classes]  # Get corresponding bird names

# Assuming you have the predicted class index in y_pred_classes
bird_name = labels[y_pred_classes[0]]
print(bird_name)

import pickle
from sklearn.preprocessing import LabelBinarizer
import numpy as np
import os  # Import os for file/directory operations

# Path to your directory containing images
path = '/content/drive/MyDrive/archive_extracted/train/*/*'  # Corrected path

# Get all file names within the bird species subdirectories
all_file_names = []
for root, dirs, files in os.walk(path):
    for file in files:
        all_file_names.append(os.path.join(root, file))



y_train = bird_species  # Use the unique bird species as your training labels

# During Training (after fitting LabelBinarizer to y_train)
lb = LabelBinarizer()
lb.fit(y_train)  # Fit to your training labels
labels = lb.classes_  # Store the class names
with open('label_binarizer.pkl', 'wb') as f:
    pickle.dump(lb, f)  # Save LabelBinarizer

# During Prediction:
with open('label_binarizer.pkl', 'rb') as f:
    lb = pickle.load(f)  # Load LabelBinarizer
labels = lb.classes_  # Access class names

# ... (Your code to make predictions using your model, resulting in y_pred) ...

# Replace with your prediction results
# Example for demonstration:
y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.1, 0.8]])

y_pred_classes = np.argmax(y_pred, axis=1)  # Predicted class indices
predicted_bird_names = [labels[i] for i in y_pred_classes]  # Get corresponding bird names

# Assuming you have the predicted class index in y_pred_classes
bird_name = labels[y_pred_classes[0]]
print(bird_name)